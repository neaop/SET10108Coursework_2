\documentclass[journal,transmag]{IEEEtran}

\usepackage{cite}
\usepackage[pdftex]{graphicx}
% declare the path(s) where your graphic files are
\graphicspath{Figures/}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png,.jpg}
\usepackage{amsmath}
\interdisplaylinepenalty=2500
\usepackage{algorithmic}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfon =sf]{subfig}
\usepackage{dblfloatfix}
\usepackage{url}
\usepackage{lipsum}
\usepackage{xcolor}
\usepackage{listings}

\lstset{
	escapeinside={/*@}{@*/},
	language=Java,	
	basicstyle=\fontsize{8.5}{12}\selectfont,
	numbers=left,
	numbersep=2pt,    
	xleftmargin=2pt,
	frame=tb,
	columns=fullflexible,
	showstringspaces=false,
	tabsize=4,
	keepspaces=true,
	showtabs=false,
	showspaces=false,
	morekeywords={inline,public,class,private,protected,struct},
	captionpos=b,
	lineskip=-0.4em,
	aboveskip=10pt,
	extendedchars=true,
	breaklines=true,
	prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
	keywordstyle=\color[rgb]{0,0,1},
	commentstyle=\color[rgb]{0.133,0.545,0.133},
	stringstyle=\color[rgb]{0.627,0.126,0.941},
}

% correct bad hyphenation here


\begin{document}

\title{An Investigation into the Parallelisation of a Ray-Tracer via the use of Threads and Distribution Methods}

\author{\IEEEauthorblockN{Sam Dixon \\ 40056761@live.napier.ac.uk}
\IEEEauthorblockA{SET10108 - Concurrent and Parallel Systems \\ School of Computing,
Edinburgh Napier University, Edinburgh}% <-this % stops an unwanted space

\thanks{Project available at: github.com/neaop/SET10108Coursework\_2}}


\markboth{SET10108 - Coursework 2}{}
% The only time the second header will appear is for the odd numbered pages after the title page when using the twoside option.

\IEEEtitleabstractindextext{


\begin{IEEEkeywords}
	C++11, Ray-Tracer, Parallel, OpenMP,Open-MPI, Thread, Distribution, Speed Up.
\end{IEEEkeywords}}

\maketitle

\IEEEdisplaynontitleabstractindextext

\IEEEpeerreviewmaketitle

\section{Introduction}
 
	\IEEEPARstart{T}{he} aim of this report is to document, analyse and compare the benefits of a number of concurrency and parallelisation techniques, and their effect upon the execution time of a C++11 implementation of a genetic string matching algorithm.
	
	\subsection{Ray-Tracing}
		\lipsum[1]
	
	\subsection{OpenMP}
		\lipsum[2]
	
	\subsection{Open-MPI}
		\lipsum[3]	
		
\section{Methodology}
	
	
	\subsection{Profiling}
		Prior to implementing any methods that will save time, the sequential code must first be analysed. By using the Visual Studio Performance Profiler, it is possible to evaluate the sequential code and locate the functions or methods that use up the most CPU time. Once the potentially problematic areas have been identified, a suitable parallelisation method can be implemented to reduce the impact of those areas on the execution time. It should be noted that all code presented in the report was run with all compiler optimisation turned off. 	
		
	\subsection{Data Collection}
		To ensure fair comparison and accurate results, each implementation was tested using the same parameters. Each solution was run for two-thousand and forty generations and the execution time was recorded. This was then repeated one-hundred times for each application and the results were then averaged. The various techniques were all tested using four threads, then again using eight threads to investigate if the use of virtual cores would have an impact upon the speed of execution. All code was benchmarked on the same PC, the specifications of which can be seen in Table \ref{pcSpecsTabel}, Page \pageref{pcSpecsTabel}. As well as the average execution time, speed-up and efficiency are calculated for each technique. Speed-up is defined as: 
		\[S=\frac{s_{t}}{p_{t}}\]
		With \(s_{t}\) being sequential time and \(p_{t}\) being parallel time.
		Efficiency is calculated as 
		\[E = \frac{S}{P}\]
		\(S\) being speed-up from the previous formula and \(P\) is the number of physical cores on the CPU.
		\begin{table}[]
			\centering
			\caption{PC Specifications}
			\label{pcSpecsTabel}
			\begin{tabular}{|l|l|}
				\hline
				CPU & i7-4790k 4 Core HT @ 4.00 ghz \\ \hline
				RAM & 16gb Dual Channel DDR3        \\ \hline
				GPU & Nvidia GeForce GTX 980        \\ \hline
				OS  & Windows 7 64 Bit              \\ \hline
			\end{tabular}
		\end{table}
		
	\subsection{OpenMP}
		OpenMP (Open Multi-Processing) is an open source API that allows for the implementation of shared memory multiprocessing with minimal developmental effort. OpenMP makes use of the C++ \#pragma directive and the pre-processor to allow developers to flag sectors of code to be parallelised. A number of different parallelisation and scheduling options can be implemented to alter the way OpenMP effects code. The OpenMP options for parallelising the code include: parallel for, static scheduling, dynamic scheduling and guided scheduling.   
	
	\subsection{Parallel For}
		The parallel for flag instructs OpenMP to split a sequential for loop’s workload over multiple threads. There are a number of caveats a developer must be aware of to implement parallel for. Firstly, the number of loop iterations must be determined at compilation time. Secondly, developers must also take care to protect any data shared across multiple threads, this can be implemented easily using OpenMP’s shared flag. The parallel for flag will be used in conjunction with scheduling flags listed below.
	
	\subsection{Static Scheduling}
		Static Scheduling is where the workload of a for loop is split into predetermined chunks which are then assigned to threads in a round-robin style. Typically, the workload of the loop is distributed as:
		\(Number Of Iterations / Number Of Threads\),
		However, the developer is free to change this configuration should they see fit.
	
	\subsection{Dynamic Scheduling}
		Dynamic scheduling is, once again, where a loop is split into chunks of predetermined size: one by default (although the developer can opt for their own chunk size) which are then distributed to any available thread. However, the difference from static scheduling is that when a thread completes a chunk of work, it will request a new chunk work to process i.e. First in, first out.
	
	\begin{table}[]
		\centering
		\caption{Table Depicting each technique, the number of threads and the average execution time, Speed-up and Efficiency}
		\label{runTimeTable}
		\begin{tabular}{|l|r|r|r|r|}
			\hline
			\textbf{Method}    & \multicolumn{1}{l|}{\textbf{Threads}} & \multicolumn{1}{l|}{\textbf{Time(ms)}} & \multicolumn{1}{l|}{\textbf{Speed-up}} & \multicolumn{1}{l|}{\textbf{Efficency}} \\ \hline
			Dynamic            & 8                                     & 27077.73                               & 1.427588                               & 0.356897                                \\ \hline
			Guided             & 8                                     & 27250.29                               & 1.418548                               & 0.354637                                \\ \hline
			Static Interleaved & 8                                     & 27545.24                               & 1.403358                               & 0.350840                                \\ \hline
			Static             & 8                                     & 27675.49                               & 1.396754                               & 0.349188                                \\ \hline
			Guided             & 4                                     & 27963.20                               & 1.382383                               & 0.345596                                \\ \hline
			Dynamic            & 4                                     & 28016.61                               & 1.379747                               & 0.344937                                \\ \hline
			Static Interleaved & 4                                     & 28537.99                               & 1.354540                               & 0.338635                                \\ \hline
			Static             & 4                                     & 29107.61                               & 1.328032                               & 0.332008                                \\ \hline
			Threads            & 4                                     & 29493.40                               & 1.310661                               & 0.327665                                \\ \hline
			Threads            & 8                                     & 29574.82                               & 1.307052                               & 0.326763                                \\ \hline
			Sequential         & 1                                     & 38655.84                               & \multicolumn{1}{l|}{NA}                & \multicolumn{1}{l|}{NA}                 \\ \hline
		\end{tabular}
	\end{table}
	
\section{Results}
	\subsection{Sequential}
		The only changes made to the provided sequential code were as follows: a for loop to allow multiple executions and a data collection function allowing the time stamps to be collected in a .csv file. It should be noted that there are several non-parallelisation techniques that could be implemented to speed up the application. These are, namely, removing console output and allowing automated compiler optimization.
	
		\subsubsection{Profiling}
			The profiling of the sequential code (Fig \ref{SeqProf}, Page \pageref{SeqProf}) revealed that the majority of the CPU’s time was spent within the update\_epoch function. Within update\_epoch CPU time was spent mainly in the epoch function – this function became the prime candidate for parallelisation. All the techniques documented hereafter were applied to epoch function, allowing a fair comparison of all methods used.\\
			It should be noted that a significant amount of CPU time was spent on the generation of random numbers and a significant speed up could be gained, should the generation be altered. This was not investigated any further during the project, however, as it would consist of a considerable amount of code refactoring, which this project aimed to avoid.
		
		\subsubsection{Execution Time}
			As stated in the methodology, the sequential solution was run one-hundred times, and the execution time recorded. Once all executions were complete these results were averaged to give us a runtime of 38655.84 milliseconds. See Table \ref{runTimeTable}, Page \pageref{runTimeTable}.\\
			It was crucial to benchmark the sequential algorithm, as it will allow for the calculation of speed-up and efficiency of the parallelisation techniques being implemented.
			
		\subsection{Parallel For}
			As determined by the profiling of the sequential code, the prime function for parallelisation is epoch. This was achieved by simply inserting the line: 
			\textbf{\#pragma omp parallel for num\_threads(MAX\_THREADS) shared(babies) schedule(schedule)}
			above the for loop present in the function. As can be seen, there are a number of variable parameters in the parallel for line that a developer can use to augment how OpenMP affects the code: \\
			\textbf{num\_threads()} allows the developer to define how many threads the for loop should be split across.\\
			\textbf{schedule()} is where a developer define which schedualing method should be used. By default OpenMP will use static.\\
			As can be seen, it takes very little effort to parallelise a for loop using OpenMP. Once the location of the parallel for had been determined, benchmarking of each scheduling flag could be performed. 
			
	\subsection{Static Schedule}
		Static schedule is the parallel for default, for this reason it was one of the first techniques to be tested. Two configurations of Static Scheduling were tested, each of four and eight threads. The first configuration (Static in Table \ref{runTimeTable}.) divides the total work load of the for loop between the available threads i.e. each thread runs one chunk of work. The other configuration (Static Interleaved in Table \ref{runTimeTable}.) has each thread run a single iteration, then wait until they are assigned another iteration to process.
		\subsubsection{Findings}
			As seen from the results table (Table \ref{runTimeTable}, Page \pageref{runTimeTable}), offers a times speed-up of at least 1.32, regardless of the number of threads, or forced interleaving.
			It should be noted that the interwoven Static schedule is consistently faster then the default Static schedule. This is likely an artefact of how OpenMP unfolds and distributes a for loop amongst threads, and is a potential area for future investigation.The thread operations for the default Static schedule can be seen in fig \ref{paraStatic8Threads}, and the interwoven in fig \ref{paraStaticI8Threads}.

	\subsection{Dynamic Schedule}
		The Dynamic Schedule has a larger overhead than Static scheduling, but has a greater potential for speed-up. After each thread has executed a chunk of iterations, one by default, it will retrieve a new chunk to process. In theory, dynamic scheduling should reduce the amount of time threads are asleep, as each thread can retrieve more work when it is ready, rather than having to wait for its turn.
		\subsubsection{Findings}
			The results show that Dynamic Scheduling with eight threads offered the most speed-up of all the methods used, with the four threaded version being the second best of all the four-threaded configurations. These results are as we expected and prove that the overall speed-up of the Dynamic schedule far outweigh the overhead required to initiate it. Should time allow, further research into optimal chunk size should be carried out, to see if any further improvement could be gained.
			Visuals of the four thread and eight thread implementation are visible in figs \ref{paraDynamic4Thread} and \ref{paraDynamic8Thread} respectively. As can be seen the eight thread version does include some sleep time due to there only being four physical cores on the CPU.
	
	\subsection{Guided Schedule}
		Guided scheduling operates in a very similar manner to Dynamic, bar the size of the work chunks. The chunks initially start quite large, but reduce in size every iteration, down to a minimum size - one by default. Typically Guided scheduling operates well on workloads that cannot be evenly dived, or that may not be finished, neither of which are the case in this scenario.
		\subsubsection{Findings}
		Interestingly, Guided Scheduling performed very well, coming in at around 170ms slower than Dynamic Scheduling with eight threads. It showed also be noted that Guided scheduling was the superior out of all the four thread configurations. 
	
	\subsection{C++11 Threads}
		The traditional method of parallelising code is via the use of a threading library. Threading requires significantly more effort that utilising OpenMP, a it demands the developer to protect any shared data and extract code in to chunks that can be run on individual threads. It is not expected for the hand threaded solution to perform better than the OpenMP solutions, as it is likely that OpenMP implements its own optimisation methods.
		\subsubsection{Findings}
		As expected, the threaded solutions did offer speed up, but not quite of the same calibre as the OpenMP solutions. That is not to say that hand threaded solutions should not be implemented, only that OpenMP will offer a bigger speed-up at lower effort to the developer.
		A visual representation of the threads is visible in fig \ref{threadedSingle4Threads}, as can be seen, while worker threads spend their entire duration working, the main thread has a considerable amount of synchronization to carry out.
	\newpage
\section{Conclusion}
	As can be seen from the results presented in this paper, most programs can be parallelised  with minimal effort from the developer, and still reduce the overall execution time of the system. Out of all the methods implemented OpenMP Parallel For with Dynamic Scheduling offered the best results in terms of speed-up and requires little effort to implement.
	
	\subsubsection{Future Work}
		Areas that could be investigated further in the future include; superior random number generation, parallelisation of multiple for loops, SIMD and Futures.
	

	\appendices

\renewcommand\refname{Bibliography}
\bibliographystyle{IEEEtran}
\bibliography{Bibliography}
\nocite{Williams:1483005}
\end{document}